---
title: "Section 3: Anomaly Detection"
output: 
  github_document:
    toc: TRUE
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Introduction

Anomalies are deviations from the norm or our expectations.

For modeling a time-series we need to find a stationary representation.
If we have a non-stationary series (e.g., always growing), each new value
may be outside of the bounds of the past observations. In this case we
cannot build an expectation as a summary of past observations without including
the growth rate. The first difference of such a series may be stationary, in which
case we can model the fluctuations about its growth path.

The rest of this discussion focuses on the case of stationary series.

The general goal is

1. Form a prediction/expected range
2. Flag individuals that are outside of the expected range

## Problem

The problem we will look at in this section is identifying anomalies
in time series. Time series can be represented in the time dimension,
the frequency dimension, or via delay embeddings. We'll focus on 
the time dimension for this section.

## Data

The date we'll use is the [NYC vehicle collision data set available on
Kaggle](https://www.kaggle.com/nypd/vehicle-collisions). 

```{r}
rm(list=ls())
library(tidyverse)
collisions <- read_csv('data/vehicle-collisions.csv')
```

### Data Summary

Let's see the bounds of the time series and pick a range for our test set.

```{r}
library(lubridate)
collisions <- collisions %>%
  mutate(
    date = mdy(DATE),
    day_of_week = wday(date, label = TRUE),
    day_of_month = mday(date)
    )
collisions %>%
  summarise(start = min(date), end = max(date))
```

For this section, we will use 2015 for our training set. Having a full year of
data is useful for finding normal bounds within the seasonality, which is most
likely present in this data.

### Training Set

Let's create our `training` set and check the seasonality assumption.

```{r}
training <- collisions %>%
  filter(date < ymd("2016-01-01"))
training_months <- training %>%
  mutate(month = as.factor(month(date, label = TRUE))) %>%
  group_by(month) %>%
  summarize(
    fatalities = sum(`PERSONS KILLED`) / days_in_month(first(date)),
    collisions = n() / days_in_month(first(date)),
    death_rate = fatalities / collisions)

training_months %>%
  ggplot(aes(month, collisions)) + geom_col() + ylim(0,NA) + 
  ggtitle("Average number of collisions per day")
```

What about the number of fatalities?

```{r}
ggplot(training_months, aes(month, fatalities)) + geom_col() + ylim(0,NA) + 
  ggtitle("Average number of fatalities per day")
```

Does there appear to be seasonality in the rate of fatalities per collision?

```{r}
ggplot(training_months, aes(month, death_rate)) + geom_col() + ylim(0,NA) + 
  ggtitle("Death by collision rate")
```


To accurately assess the level of seasonality, it would be best for us to have
multiple years. To avoid overfitting, we will live within the limits of only
using one year of training data. It is clear from even this small sample that
had we only selected January through March we would underestimate the number of
traffic fatalities for all but September.

Let's see if there appears to be a pattern with respect to days of the week
and days of the month.

```{r}
training_days <- training %>%
  group_by(date) %>%
  summarize(
    fatalities = sum(`PERSONS KILLED`),
    day_of_week = first(day_of_week)
    )
ggplot(training_days, aes(day_of_week, fatalities)) + geom_violin()
```

Total deaths by day of the week:

```{r}
ggplot(training_days, aes(day_of_week, fatalities)) + geom_col()
```

And now fatalities by day of the month:

```{r}
training_dates <- training %>%
  group_by(day_of_month) %>%
  summarize(fatalities = sum(`PERSONS KILLED`))

ggplot(training_dates, 
       aes(((day_of_month - 1) %% 7) + 1, ((day_of_month - 1) %/% 7) + 1)) +
  geom_raster(aes(fill = fatalities)) + ylim(5,0) +
  ylab("") + xlab("") + ggtitle("Calendar Heatmap")
```

## Modeling 

Let's start by predicting the number of collisions per day.
```{r}
training_series <- training %>%
  group_by(date) %>%
  summarize(
    collisions = n(), 
    injuries = sum(`PERSONS INJURED`), 
    fatalities = sum(`PERSONS KILLED`)
    )

ggplot(training_series, aes(date, collisions)) + geom_line()
```


### Naive Model

A simple approach is to assume a roughly Gaussian process and to
use a rule that we want to flag observations that are very unlikely.

Let's see how Gaussian this data is:

```{r}
ggplot(training_series, aes(collisions)) + geom_density() +
  stat_function(fun=dnorm, color = "red", args = list(mean = mean(training_series$collisions), sd = sd(training_series$collisions)))
```

For rarer events, we'll predict the number of injuries (or fatalities).

A simple anomaly detector can flag days where the observations
are greater than x standard deviations from the mean.

```{r}
mean_collisions = mean(training_series$collisions)
sd_collisions = sd(training_series$collisions)
moderate_collisions = mean_collisions + sd_collisions
high_collisions = mean_collisions + 2 * sd_collisions
extreme_collisions = mean_collisions + 3 * sd_collisions

ggplot(training_series, aes(date, collisions)) + 
  geom_hline(yintercept = moderate_collisions, color = "yellow") +
  geom_hline(yintercept = high_collisions, color = "orange") +
  geom_hline(yintercept = extreme_collisions, color = "red") +
  geom_line() +
  geom_point(data = training_series[training_series$collisions > extreme_collisions,], color = "red")
```

## Twitter's Anomaly Detection Library

See a blog post of this [here](https://blog.twitter.com/engineering/en_us/a/2015/introducing-practical-and-robust-anomaly-detection-in-a-time-series.html)
and the paper describing their algorithm [here](https://www.usenix.org/system/files/conference/hotcloud14/hotcloud14-vallis.pdf).
A common approach to time series modeling is Seasonal and Trend decomposition with LOESS ([Chapter on this topic](https://www.otexts.org/fpp/6/5)). The S-H-ESD algorithm in Twitter's library
uses a piece-wise median

Install using the following commands
```
install.packages("devtools")
devtools::install_github("twitter/AnomalyDetection")
```

```{r}
library(AnomalyDetection)
ts_result = training_series %>%
  mutate(datetime = as_datetime(date)) %>%
  select(datetime, collisions) %>%
  AnomalyDetectionTs(max_anoms=0.02, longterm = TRUE, piecewise_median_period_weeks = 8, direction='pos', plot=TRUE)
ts_result$plot
```

```{r}
cbind(date = as.character(ts_result$anoms$timestamp), anoms = ts_result$anoms$anoms)
```

## Anomalous

```{r}
#devtools::install_github("robjhyndman/anomalous")
library(anomalous)
y <- tsmeasures(training_series %>% select(collisions, injuries, fatalities))
y
biplot(y)
```

## tsoutliers

`tsoutliers` is another library providing outlier detection based on the methods of Chen and Liu (1993)
```{r}
#install.packages("tsoutliers")
library(tsoutliers)

collisions_ts <- ts(training_series$collisions, c(2015, 1), frequency = 365)

collisions_outliers <- tso(y = collisions_ts)
collisions_outliers
```

Type `AO` stands for Additive Outlier.

`tsoutliers` also provides a handy visualization of the identified outlier.
```{r}
plot(collisions_outliers)
```